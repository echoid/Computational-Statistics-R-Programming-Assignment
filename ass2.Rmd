---
title: "MAST90083 Assignment2"
author: "Youran Zhou"
date: "2021/10/9"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(HRW)
library(splines)
library(pracma)
library(SpatialExtremes)
library(MASS)
library("plot.matrix")
library("png")
library("fields")
```


```{r}
data(WarsawApts)
head(WarsawApts)
WarsawApts <- WarsawApts[order(WarsawApts$construction.date),]

n = dim(WarsawApts)[1]
```

# Question 1

## (1)

```{r}

x = WarsawApts$construction.date

y = WarsawApts$areaPerMzloty


x_unique = unique(x)
  
a = seq(0,1,length=22)[2:21]

#(k = quantile(x,a))
(k = quantile(x_unique,a))

```
## (2)

```{r}
Splus = function(x, knot){
  
  result_matrix = matrix(0, length(x), length(knot))
  
  for (i in 1:length(knot)){
    
    for (j in 1:length(x)){
      
      result = x[j] - knot[i]
      
      
      if (result <= 0){
          result = 0
      }
      
      result_matrix[j,i] = result
      
    }

  }
  return(result_matrix)

}

```

Z is a matrix contains 20 basis function.
$$
Z = \left[
\begin{array}{c}
(x-k_1)_{+}  \\
(x-k_2)_{+}\\
\vdots\\
(x-k_{20})_{+}
\end{array}
\right]
$$





```{r}
x_range = seq(min(x)-200,max(x)+200,length=1000)

plot_Z = Splus(x_range,k)
```


```{r}
plot(x_range,plot_Z[,1],typ = "l",ylim = c(-1,2),xlim = c(min(x),max(x)),main = "Z matrix",xlab = "construction date(year)", ylab ="linear spline basis" )

for (i in 2:dim(plot_Z)[2]){
    lines(x_range,plot_Z[,i])

}

```

## (3)


```{r}
Z = Splus(x,k)
ones = rep(1,n)

C = cbind(ones,x,Z)

D = diag(1, length(k)+2,length(k)+2)
D[1,1] = 0
D[2,2] = 0

lambda = seq(0,50,length=100)

```

```{r}
fit_spline = function(D,C,y,lambda){
  fit_y= matrix(0, nrow = length(y), ncol = length(lambda))
  fit_df = c()
  fit_RSS = c()
  fit_GCV = c()
  for (i in 1:length(lambda)){
    
    inver = solve(t(C) %*% C + lambda[i] * D)
    
    predict_y = C%*% inver %*% t(C) %*% y
    
    df = sum(diag(inver %*% t(C) %*% C))
    
    RSS = sum((predict_y - y)^2)
    
    GCV = RSS / (1- df/n)^2
    
    
    fit_y[,i] = predict_y
    fit_df = c(fit_df, df)
    fit_RSS = c(fit_RSS, RSS)
    fit_GCV = c(fit_GCV, GCV)
    
  }
  
  
  return(list(fit_y = fit_y,fit_df = fit_df, fit_RSS = fit_RSS, fit_GCV = fit_GCV))
  
}
 
```

```{r}
fitted = fit_spline(D,C,y,lambda)
```

## (4)

```{r}
fitted$fit_RSS
```
```{r}
fitted$fit_df
```

```{r}
fitted$fit_GCV
```

## (5)


```{r}
min = which(fitted$fit_GCV == min(fitted$fit_GCV))

(min_lambda = lambda[min])

(MSE = fitted$fit_RSS[min]/n)
```
```{r}
plot(x,y,xlab = "x",ylab = "y",main = "Fitted Spline")


lines(x,fitted$fit_y[,min],col = 2)

```
### 60 Basis

```{r}

a = seq(0,1,length=62)[2:61]

(k_60 = quantile(x_unique,a))

#(k_60 = quantile(x,a))


```

```{r}
plot_Z_60 = Splus(x_range,k_60)

plot(x_range,plot_Z_60[,1],typ = "l",ylim = c(-1,2),xlim = c(min(x),max(x)),main = "Z matrix",xlab = "construction date(year)", ylab ="linear spline basis" )

for (i in 2:dim(plot_Z_60)[2]){
    lines(x_range,plot_Z_60[,i])

}
```

```{r}
Z_60 = Splus(x,k_60)


C_60 = cbind(ones,x,Z_60)

D_60 = diag(1, length(k_60)+2,length(k_60)+2)
D_60[1,1] = 0
D_60[2,2] = 0

```


```{r}
fitted_60 = fit_spline(D_60,C_60,y,lambda)
```

```{r}

min_60 = which(fitted_60$fit_GCV == min(fitted_60$fit_GCV))

(min_lambda_60 = lambda[min_60])

(MSE_60 = fitted_60$fit_RSS[min_60]/n)

```

```{r}
plot(x,y,xlab = "x",ylab = "y",main = "Fitted Spline (60 basis)")

lines(x,fitted_60$fit_y[,min_lambda_60],col=2)

```


```{r}
(MSE)
(MSE_60)
plot(x,y,xlab = "x",ylab = "y",main = "Fitted Spline (20 basis vs 60 basis)")

lines(x,fitted_60$fit_y[,min_lambda_60],col=2)
lines(x,fitted$fit_y[,min_lambda],col=3)
```

Increasing the number of basis does not help to improve the result.


# Question 2

## (1)

```{r}
k_list = c(3,6,9,12,15,18)

k_list = c(3,7,11,15,19,23)
```

```{r}
KNN = function(k,current_X, X){
  
  d = abs(current_X - X)
  
  index = sort(d, index.return=TRUE)$ix[1:k]
  
  return(mean(y[index]))
}
```

## (2)

```{r}
KNN_prediction = matrix(0,length(x),length(k_list))
KNN_MSE = c()
colnames(KNN_prediction) = k_list

for (i in 1:length(k_list)){
  predict_y = c()
  for (current_x in x){
    
    predict_y = c(predict_y, KNN(k_list[i],current_x,x))
    
  }
  
  KNN_prediction[,i] = predict_y
  
  KNN_MSE = c(KNN_MSE,sum((predict_y - y)^2)/n)
  
}


```

```{r}
par(mfrow=c(3,2))

plot(x,y,main = "Knn with K = 3")
lines(x,KNN_prediction[,1],col = 2)

plot(x,y,main = "Knn with K = 7")
lines(x,KNN_prediction[,2],col = 2)

plot(x,y,main = "Knn with K = 11")
lines(x,KNN_prediction[,3],col = 2)

plot(x,y,main = "Knn with K = 15")
lines(x,KNN_prediction[,4],col = 2)

plot(x,y,main = "Knn with K = 19")
lines(x,KNN_prediction[,5],col = 2)

plot(x,y,main = "Knn with K = 23")
lines(x,KNN_prediction[,6],col = 2)

```

```{r}
rbind(k_list,KNN_MSE)
```
When K = 7, we have the minimum MSE. Overall there is not a significant difference between using KNN and spline, but we can choose the different values of K to get slightly different results. Choosing an approximate K value could help us to get a better result. From the result we could say as the K value increases, the result is getting worse. However, the K value can not be too small since it will easily be affected by noisy data.

## (3)

```{r}
accommodate_Kernels = function(x){
  if (abs(x) < 1) {
    
    Epanechnikov = 3/4*(1-x^2)
    Biweight     = 15/16*(1-x^2)^2
    Triweight    = 35/32*(1-x^2)^3
    Uniform      = 1/2
    Tricube      = 70/81*(1-(abs(x))^3)^3
    
  }else{
    
    Epanechnikov = 0
    Biweight     = 0
    Triweight    = 0
    Uniform      = 0
    Tricube      = 0
  }
  
    Gaussian = (2*pi)^(-1/2)*exp(-x^2/2)
    
    return(c(Epanechnikov,Gaussian,Biweight,Triweight, Uniform, Tricube))
}
```


## (4)

```{r}
h = 2
K_h = function(x_i,x_j){
  return((x_j - x_i)/h )
}
```

```{r}

Kernel_predict = matrix(0,length(x),6)

colnames(Kernel_predict) <- c("Epanechnikov","Gaussian","Biweight","Triweight","Uniform","Tricube")

for (i in 1:length(x)){
  Kernel_weight = matrix(0,length(x),6)
  numerator = 0
   
  for (j in 1:length(x)){
    
    value =  K_h(x[i],x[j])
    weight = accommodate_Kernels(value)
    numerator = numerator + (weight * y[j])
    Kernel_weight[j,] = accommodate_Kernels(value)
  }
  predict_y = numerator / colSums(Kernel_weight)
  
  
  Kernel_predict[i,] = predict_y
  
}

```


```{r}
Kernel_predict[1:10,]
```
## (5)


```{r}
par(mfrow=c(3,2))

plot(x,y,main = "Epanechnikov Kernel")
lines(x,Kernel_predict[,1],col = 2)

plot(x,y,main = "Gaussian Kernel")
lines(x,Kernel_predict[,2],col = 2)

plot(x,y,main = "Biweight Kernel")
lines(x,Kernel_predict[,3],col = 2)

plot(x,y,main = "Triweight Kernel")
lines(x,Kernel_predict[,4],col = 2)

plot(x,y,main = "Uniform Kernel")
lines(x,Kernel_predict[,5],col = 2)

plot(x,y,main = "Tricube Kernel")
lines(x,Kernel_predict[,6],col = 2)
```


```{r}
(Kernel_MSE = colSums((Kernel_predict - y)^2)/n)
```
Triweight Kernel could provide the minimum MSE.

# Question 3


## (1)


```{r}
I = readPNG("CM.png")
I = I[,,1] # only first channel
I = t(apply(I, 2, rev))
par(mfrow = c(1,2))
image(I,col = gray((0:255)/255))
plot(density(I))
```

## (2)

```{r}
lappend <- function (lst, ...){
lst <- c(lst, list(...))
  return(lst)
}
```



```{r}
# The source code comes from MAST30027, edit by me
EM_algo <- function(X, p, mu,sigma, epsilon=1e-6) {
  
  
  
  m_list = list(mu)
  c_list = list(sigma)
  e_list = list(0)
    
  
  criteria = 1


  n = 1
  
  
  while((criteria > epsilon)){

    
    n = n + 1

    result = EM_step(X, p, mu, sigma)

    p = result$w.new
    mu = result$mu.new
    sigma = result$sigma.new
    
    m_list = lappend(m_list,mu)
    c_list = lappend(c_list,sigma)
    

    
    e = sum(m_list[n][[1]] - m_list[n-1][[1]]) + 
        sum(c_list[n][[1]] - c_list[n-1][[1]]) 
    
    
    
    e_list = lappend(e_list,e)
    
    criteria = abs(e - e_list[n-1][[1]])

   
    
    print(round(c(mu,sigma,p),4))
    
  }
  
  return(list(mu=mu, sigma = sigma, p = p))
}
```

```{r}
EM_step <- function(X, p, mu,sigma) {
  

  # E-step:
  
  prob.y_z = Prob.y_z(X, p, mu,sigma)
  
  # compute posterior P(Z=k|y) 
  posterior = prob.y_z / rowSums(prob.y_z)
  
  # M-step
  w.new = colSums(posterior)/sum(posterior)  
  
  mu.new = colSums(posterior*X)/colSums(posterior)
  
  sigma.new = update_sigma(posterior,mu.new,X)


  return(list(w.new=w.new, mu.new=mu.new, sigma.new = sigma.new))
}
```



```{r}
# compute P(y, Z=k)
Prob.y_z <- function(X, p, mu,sigma) {

  L = matrix(NA, nrow=length(X), ncol= length(p))
  for(k in seq_len(ncol(L))) {

    L[, k] = dnorm(X, mean=mu[k], sd=sigma[k])*p[k]
  }
  
  return(L)
}
```


```{r}
update_sigma = function(p,mu,X){
  
  
  mu_matrix = t(replicate(length(X),mu))
  X_matrix = replicate(length(mu),X)
  update_P = p * (X_matrix - mu_matrix)^2
  
  return(sqrt(colSums(update_P)/colSums(p)))
  
}
```


## (3)

```{r}
EM <- EM_algo(sort(c(I)), 
              p=c(0.3,0.3, 0.4), 
              mu=c(0, 0.5, 1),
              sigma=c(0.1, 0.1, 0.1), 
              epsilon=1e-6)
```



```{r}
EM
```
## (4)
```{r}
EM_result = c()

for (i in sort(c(I))){
  
  EM_result = c( EM_result, 
       dnorm(i,mean = EM$mu[1],sd = EM$sigma[1]) * EM$p[1] +   dnorm(i,mean = EM$mu[2],sd = EM$sigma[2])* EM$p[2] 
  + dnorm(i,mean = EM$mu[3],sd = EM$sigma[3])*EM$p[3])
}


```


```{r}

plot(density(I), main = "Density function")
lines(sort(c(I)),EM_result,lty = 1,col = 2)
legend(0,5.5,c("Density","EM"), lwd=c(2,2), col=c("black",2), y.intersp=1.5)

```


```{r}
z_predict = data.frame(Prob.y_z(c(I), EM$p, EM$mu,EM$sigma))
image1 = apply(z_predict,1,which.max)


image1[image1 == 1] = 0
image1[image1 == 2] = 8
image1[image1 == 3] = 4


```


```{r}
posterior = z_predict / rowSums(z_predict)
mu = t(replicate(length(c(I)),EM$mu))

image2 = rowSums(posterior * mu)
```


## (5)

```{r}
image.plot(matrix(image1,nrow = 398,ncol = 398))
```


```{r}


image.plot(matrix(image2,nrow = 398,ncol = 398))

```
